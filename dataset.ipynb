{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1dbfd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a5d38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check if pytesseract is available\n",
    "TESSERACT_AVAILABLE = False\n",
    "try:\n",
    "    import pytesseract\n",
    "    from PIL import Image\n",
    "    TESSERACT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: pytesseract or PIL is not installed. OCR for formulas in images will be disabled.\")\n",
    "    print(\"To enable this feature, install with: pip install pytesseract pillow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e04f28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 10_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7cf3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def is_likely_formula(text_block):\n",
    "    \"\"\"Check if a text block is likely to contain a formula.\"\"\"\n",
    "    formula_indicators = [\n",
    "        r'[=×∙÷+\\-*/^√∫∑∏≈<>≤≥]',  # Mathematical operators\n",
    "        r'[α-ωΑ-Ω]',  # Greek letters\n",
    "        r'\\b[a-zA-Z]\\s*\\^\\s*[0-9]',  # Simple exponents like x^2\n",
    "        r'\\bsin\\b|\\bcos\\b|\\btan\\b|\\blog\\b|\\bln\\b|\\blim\\b|\\bmax\\b|\\bmin\\b',  # Function names\n",
    "        r'\\bexp\\b|\\bsqrt\\b|\\bfrac\\b',  # More function names\n",
    "        r'\\(.*[=+\\-*/^].*\\)'  # Expressions in parentheses with operators\n",
    "    ]\n",
    "    \n",
    "    # Check if any of the formula indicators are present\n",
    "    for pattern in formula_indicators:\n",
    "        if re.search(pattern, text_block):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def process_formula(formula_text):\n",
    "    \"\"\"Process formula text to make it more suitable for training.\"\"\"\n",
    "    # Replace common math symbols with LaTeX equivalents\n",
    "    replacements = {\n",
    "        '×': ' \\\\times ',\n",
    "        '÷': ' \\\\div ',\n",
    "        '≈': ' \\\\approx ',\n",
    "        '≤': ' \\\\leq ',\n",
    "        '≥': ' \\\\geq ',\n",
    "        '∑': ' \\\\sum ',\n",
    "        '∫': ' \\\\int ',\n",
    "        '∏': ' \\\\prod ',\n",
    "        '√': ' \\\\sqrt ',\n",
    "        'π': ' \\\\pi ',\n",
    "        'θ': ' \\\\theta ',\n",
    "        'α': ' \\\\alpha ',\n",
    "        'β': ' \\\\beta ',\n",
    "        'γ': ' \\\\gamma ',\n",
    "        'δ': ' \\\\delta ',\n",
    "        'ε': ' \\\\epsilon ',\n",
    "        'λ': ' \\\\lambda ',\n",
    "        'μ': ' \\\\mu ',\n",
    "        'σ': ' \\\\sigma ',\n",
    "        'τ': ' \\\\tau ',\n",
    "        'ω': ' \\\\omega ',\n",
    "        '∞': ' \\\\infty '\n",
    "    }\n",
    "    \n",
    "    # Apply replacements\n",
    "    for orig, repl in replacements.items():\n",
    "        formula_text = formula_text.replace(orig, repl)\n",
    "    \n",
    "    # Convert superscripts (e.g., x² to x^2)\n",
    "    formula_text = re.sub(r'([a-zA-Z0-9])²', r'\\1^2', formula_text)\n",
    "    formula_text = re.sub(r'([a-zA-Z0-9])³', r'\\1^3', formula_text)\n",
    "    \n",
    "    # Wrap the formula in delimiters to indicate it's a formula\n",
    "    return f\"[FORMULA] {formula_text} [/FORMULA]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c03944",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_text_and_formulas_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text and formulas from a PDF file with special handling for formulas.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        result_text = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            page_text = \"\"\n",
    "            \n",
    "            # Extract all blocks of text\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            \n",
    "            for block in blocks:\n",
    "                block_text = block[4]\n",
    "                \n",
    "                if is_likely_formula(block_text):\n",
    "                    # Process and mark as formula\n",
    "                    processed_formula = process_formula(block_text)\n",
    "                    page_text += processed_formula + \"\\n\"\n",
    "                else:\n",
    "                    # Regular text\n",
    "                    page_text += block_text + \"\\n\"\n",
    "            \n",
    "            # If Tesseract is available, try to process images that might contain formulas\n",
    "            if TESSERACT_AVAILABLE:\n",
    "                try:\n",
    "                    # Get images that might contain formulas\n",
    "                    image_list = page.get_images(full=True)\n",
    "                    \n",
    "                    # If there are images, check for formulas in them\n",
    "                    if image_list:\n",
    "                        for img_index, img_info in enumerate(image_list):\n",
    "                            xref = img_info[0]\n",
    "                            base_image = doc.extract_image(xref)\n",
    "                            image_bytes = base_image[\"image\"]\n",
    "                            \n",
    "                            # Convert image bytes to PIL Image\n",
    "                            img = Image.open(io.BytesIO(image_bytes))\n",
    "                            \n",
    "                            # Try to extract text from the image (might be a formula)\n",
    "                            img_text = pytesseract.image_to_string(img, config='--psm 6')\n",
    "                            \n",
    "                            # If the extracted text looks like a formula, add it\n",
    "                            if img_text.strip() and is_likely_formula(img_text):\n",
    "                                processed_formula = process_formula(img_text)\n",
    "                                page_text += processed_formula + \"\\n\"\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image in {pdf_path}, page {page_num+1}: {e}\")\n",
    "            \n",
    "            result_text.append(page_text)\n",
    "        \n",
    "        return \"\\n\".join(result_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ed67a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean the extracted text while preserving formula markers.\"\"\"\n",
    "    # First, protect formula sections by replacing them temporarily\n",
    "    formula_sections = []\n",
    "    \n",
    "    def replace_formula(match):\n",
    "        formula_sections.append(match.group(0))\n",
    "        return f\"[FORMULA_PLACEHOLDER_{len(formula_sections)-1}]\"\n",
    "    \n",
    "    # Find all formula sections and store them\n",
    "    protected_text = re.sub(r'\\[FORMULA\\].*?\\[/FORMULA\\]', replace_formula, text, flags=re.DOTALL)\n",
    "    \n",
    "    # Now clean the text\n",
    "    cleaned_text = protected_text\n",
    "    # Replace multiple newlines with a single newline\n",
    "    cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    # Remove any non-printable characters (except for formula placeholders)\n",
    "    cleaned_text = re.sub(r'[^\\x20-\\x7E\\n\\[FORMULA_PLACEHOLDER_\\d+\\]]', '', cleaned_text)\n",
    "    \n",
    "    # Put back the formula sections\n",
    "    for i in range(len(formula_sections)):\n",
    "        placeholder = f\"[FORMULA_PLACEHOLDER_{i}]\"\n",
    "        if placeholder in cleaned_text:\n",
    "            cleaned_text = cleaned_text.replace(placeholder, formula_sections[i])\n",
    "    \n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affb8dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def split_into_chunks_preserving_formulas(text, max_chunk_size=1024):\n",
    "    \"\"\"Split text into chunks, preserving formulas and trying to respect sentence boundaries.\"\"\"\n",
    "    # First, identify formula sections and protect them\n",
    "    formula_pattern = r'\\[FORMULA\\].*?\\[/FORMULA\\]'\n",
    "    formulas = re.finditer(formula_pattern, text, re.DOTALL)\n",
    "    formula_positions = [(m.start(), m.end(), m.group(0)) for m in formulas]\n",
    "    \n",
    "    # Split text into sentences\n",
    "    doc = nlp(text)\n",
    "    sentences = []\n",
    "    last_pos = 0\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.strip()\n",
    "        if not sent_text:\n",
    "            continue\n",
    "\n",
    "        sent_start = text.find(sent_text, last_pos)\n",
    "        if sent_start == -1:\n",
    "            sentences.append(sent_text)\n",
    "            continue\n",
    "    \n",
    "        sent_end = sent_start + len(sent)\n",
    "        relevant_formulas = [f for f in formula_positions if \n",
    "                            (f[0] >= sent_start and f[0] < sent_end) or  \n",
    "                            (f[1] > sent_start and f[1] <= sent_end) or  \n",
    "                            (f[0] <= sent_start and f[1] >= sent_end)]\n",
    "        \n",
    "        sentences.append(sent_text)\n",
    "        last_pos = sent_end\n",
    "    \n",
    "    # Now create chunks, being careful not to split formulas\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        new_chunk_len = len(current_chunk) + len(sentence)\n",
    "        if new_chunk_len > max_chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a816b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def process_pdfs(search_dir=\".\", output_dir=\"training_data\", max_chunk_size=1024):\n",
    "    \"\"\"\n",
    "    Search for training_data folder, process PDFs, and save text chunks with special handling for formulas.\n",
    "    \"\"\"\n",
    "    # Find training_data directory\n",
    "    training_data_dir = None\n",
    "    for root, dirs, _ in os.walk(search_dir):\n",
    "        if \"training_data\" in dirs:\n",
    "            training_data_dir = os.path.join(root, \"training_data\")\n",
    "            break\n",
    "    \n",
    "    if not training_data_dir:\n",
    "        print(\"No 'training_data' directory found. Creating one in current directory.\")\n",
    "        training_data_dir = os.path.join(search_dir, \"training_data\")\n",
    "        os.makedirs(training_data_dir, exist_ok=True)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find PDF files in training_data directory\n",
    "    pdf_files = []\n",
    "    for extension in ['pdf', 'PDF']:\n",
    "        pdf_files.extend(glob.glob(os.path.join(training_data_dir, f\"**/*.{extension}\"), recursive=True))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {training_data_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files in {training_data_dir}\")\n",
    "    \n",
    "    # Create a metadata file for tracking processed files\n",
    "    metadata_file = os.path.join(output_dir, \"metadata.json\")\n",
    "    metadata = {}\n",
    "    chunk_count = 0\n",
    "    formula_count = 0\n",
    "    \n",
    "    # Process each PDF file\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        pdf_name = os.path.basename(pdf_path)\n",
    "        print(f\"Processing {pdf_name}...\")\n",
    "        \n",
    "        # Extract text with formula handling\n",
    "        raw_text = extract_text_and_formulas_from_pdf(pdf_path)\n",
    "        if not raw_text:\n",
    "            print(f\"Skipping {pdf_name} - no text extracted\")\n",
    "            continue\n",
    "        \n",
    "        # Count formulas\n",
    "        formula_matches = re.findall(r'\\[FORMULA\\].*?\\[/FORMULA\\]', raw_text, re.DOTALL)\n",
    "        current_formula_count = len(formula_matches)\n",
    "        formula_count += current_formula_count\n",
    "        \n",
    "        # Clean text while preserving formulas\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        \n",
    "        # Split into chunks, being careful with formulas\n",
    "        chunks = split_into_chunks_preserving_formulas(cleaned_text, max_chunk_size)\n",
    "        \n",
    "        # Save chunks\n",
    "        file_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_filename = f\"{pdf_name.replace('.pdf', '').replace('.PDF', '')}_chunk_{i+1}.txt\"\n",
    "            chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "            \n",
    "            with open(chunk_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(chunk)\n",
    "            \n",
    "            # Count formulas in this chunk\n",
    "            chunk_formula_count = len(re.findall(r'\\[FORMULA\\].*?\\[/FORMULA\\]', chunk, re.DOTALL))\n",
    "            \n",
    "            file_chunks.append({\n",
    "                \"chunk_id\": i+1,\n",
    "                \"filename\": chunk_filename,\n",
    "                \"characters\": len(chunk),\n",
    "                \"formulas\": chunk_formula_count\n",
    "            })\n",
    "            chunk_count += 1\n",
    "        \n",
    "        # Update metadata\n",
    "        metadata[pdf_name] = {\n",
    "            \"original_path\": pdf_path,\n",
    "            \"chunks\": file_chunks,\n",
    "            \"total_chunks\": len(chunks),\n",
    "            \"total_formulas\": current_formula_count\n",
    "        }\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Processing complete! Created {chunk_count} text chunks from {len(pdf_files)} PDF files.\")\n",
    "    print(f\"Detected and processed {formula_count} mathematical formulas.\")\n",
    "    print(f\"Processed data saved to {output_dir}\")\n",
    "    print(f\"Metadata saved to {metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad07dca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Process PDF files into training data for language models with special handling for mathematical formulas\")\n",
    "    parser.add_argument(\"--search_dir\", type=str, default=\".\", help=\"Directory to start searching for training_data folder\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"processed_training_data\", help=\"Directory to save processed text files\")\n",
    "    parser.add_argument(\"--max_chunk_size\", type=int, default=1024, help=\"Maximum size of text chunks in characters\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    process_pdfs(args.search_dir, args.output_dir, args.max_chunk_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
